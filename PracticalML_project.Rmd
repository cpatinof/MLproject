---
title: "Practical Machine Learning Project"
author: "Carlos Ignacio Patiño"
date: "September 24, 2015"
output: html_document
---

From the project description: Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: <http://groupware.les.inf.puc-rio.br/har> (see the section on the Weight Lifting Exercise Dataset).

```{r, libraries, warning=FALSE, message=FALSE, comment="", echo=FALSE}
library(caret)
library(ggplot2)
library(plyr); library(dplyr)
```

# Loading the Data

The data for this project comes from this source: <http://groupware.les.inf.puc-rio.br/har>. The following chunk of code downloads and loads the training and testing sets in the local environment.

```{r, load, warning=FALSE, message=FALSE, comment="", echo=TRUE}
url.train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url.test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

download.file(url.train, "train.csv", method="curl")
download.file(url.test, "test.csv", method="curl")

training <- read.csv("train.csv", header=T)
testing <- read.csv("test.csv", header=T)
```

# Partitioning the Data and Pre-Processing

The data is already partitioned into training and testing sets. We will hold the testing set aside and only use it for final model validation (the second part of this project). For training, we will use 10-fold cross-validation in order to tune in the parameters and/or arguments of each potential classifier or ensemble algorithm. However, we do need to do some preprocessing in order to clean up the database as we notice that some columns show issues such as missing values or possible errors. The steps performed in this pre-processing stage are:

* Exclude indicator variables (first 7 columns of the datasets)
* Exclude near zero variance columns (a total of 94 variables, including the target, are left)
* Exclude columns with almost all instances missing (53 columns are left after this step)
* Impute remaining instances using KNN imputation, if necessary
* Create an alternative PCA set with the principal components that explain 80% of the variability in all features

```{r, preprocessing, warning=FALSE, message=FALSE, echo=FALSE}
# First we elimimate variables that are indicators (individuals, timestamps, etc)
training <- training[,-c(1:7)]
testing <- testing[,-c(1:7)]

# Near zero-variance predictors
nzv <- nearZeroVar(training)
f.training <- training[,-nzv]
f.testing <- testing[,-nzv]

# Exclude all NAs columns
f.training <- select(f.training,
                     -(max_roll_belt:var_yaw_belt),
                     -var_accel_arm,
                     -(max_picth_arm:amplitude_yaw_arm),
                     -(max_roll_dumbbell:amplitude_pitch_dumbbell),
                     -(var_accel_dumbbell:var_yaw_dumbbell),
                     -(max_picth_forearm:amplitude_pitch_forearm),
                     -var_accel_forearm)
f.testing <- select(f.testing,
                     -(max_roll_belt:var_yaw_belt),
                     -var_accel_arm,
                     -(max_picth_arm:amplitude_yaw_arm),
                     -(max_roll_dumbbell:amplitude_pitch_dumbbell),
                     -(var_accel_dumbbell:var_yaw_dumbbell),
                     -(max_picth_forearm:amplitude_pitch_forearm),
                     -var_accel_forearm)

# No need to impute as all remaining predictors are complete
test<-complete.cases(f.training)
#sum(test)

# PCA for alternative models
preProc <- preProcess(f.training[,-53], method="pca", thresh=0.8)
trainPC <- predict(preProc,f.training[,-53])
testPC <- predict(preProc,f.testing[,-53])
```

# EDA

In this section we perform a quick Exploratory Data Analysis on the training set, using the two principal components and coloring the points in the graph by the labels. We can notice a complex structure. Aparently, there are 5 clusters of cases. Given such structure, an ensemble method might be a good approach in order to train a complex decision boundary.

```{r, eda, warning=FALSE, message=FALSE, comment="", echo=FALSE}
qplot(trainPC[,1],trainPC[,2],colour=f.training$classe, data=trainPC)
```

# Models to be Tested: 10-fold CV

Given the nature of the data, we will try the following classifiers:

* Naive Bayes
* CART
* Boosting (tree-based ensemble method)
* CART with Principal Components as predictors (from PCA)

We use 10-fold Cross-Validation to fine tune the parameters of each algorithm, and then compare the out of sample performance  in order to select the best approach as our final model. We expect this out of sample error to be larger than in the case of the in-sample error.

```{r, models, warning=FALSE, message=FALSE, echo=FALSE}
# Fit control for 10-fold CV
fitControl <- trainControl(## 10-fold CV
                           method = "cv",
                           number = 10)
# Models
nb <- train(classe~., data=f.training,
            method="nb",
            trControl=fitControl)
cart <- train(classe~., data=f.training,
              method="rpart",
              trControl=fitControl)
boost <- train(classe~., data=f.training,
               method="gbm",
               trControl=fitControl,
               verbose=F)
cart.pca <- train(f.training$classe~., data=trainPC,
                  method="rpart",
                  trControl=fitControl)
```

As expected, the ensemble method Boosting shows the largest accuracy (cross-validated). The final model has the following parameters: 150 trees, interaction depth of 3 (complexity of the tree), learning rate (shrinkage) of 0.1, and minimum number of training set samples in a node equal to 10. The cross-validated accuracy reaches 0.96, meaning that the classifier correctly classifies 96% of the instances (out of sample, using CV). This out of sample accuracy is expected to be lower than the in-sample accuracy. We can test that by looking at the in-sample (training set) confusion matrix.

```{r, confMat, warning=FALSE, message=FALSE, echo=FALSE}
confusionMatrix(f.training$classe,predict(boost,f.training))
```

# Final Model

The best model is then the Boosting method, which includes the training of 150 trees and is fine tuned via 10-fold cross-validation.

```{r, finMod, warning=FALSE, message=FALSE, echo=FALSE}
boost
```

We now predict the 20 test cases using our Boosting classifier.

```{r, preds, warning=FALSE, message=FALSE, echo=FALSE}
preds <- predict(boost,f.testing)
```